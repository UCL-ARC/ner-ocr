{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR  \n",
    "\n",
    "ocr = PaddleOCR(\n",
    "    use_doc_orientation_classify=False, # Disables document orientation classification model via this parameter\n",
    "    use_doc_unwarping=False, # Disables text image rectification model via this parameter\n",
    "    use_textline_orientation=False, # Disables text line orientation classification model via this parameter\n",
    ")\n",
    "# ocr = PaddleOCR(lang=\"en\") # Uses English model by specifying language parameter\n",
    "# ocr = PaddleOCR(ocr_version=\"PP-OCRv4\") # Uses other PP-OCR versions via version parameter\n",
    "# ocr = PaddleOCR(device=\"gpu\") # Enables GPU acceleration for model inference via device parameter\n",
    "# ocr = PaddleOCR(\n",
    "#     text_detection_model_name=\"PP-OCRv5_mobile_det\",\n",
    "#     text_recognition_model_name=\"PP-OCRv5_mobile_rec\",\n",
    "#     use_doc_orientation_classify=False,\n",
    "#     use_doc_unwarping=False,\n",
    "#     use_textline_orientation=False,\n",
    "# ) # Switch to PP-OCRv5_mobile models\n",
    "result = ocr.predict(\"../data/input/sd19.jpg\")  \n",
    "for res in result:  \n",
    "    res.print()  \n",
    "    res.save_to_img(\"../data/output\")  \n",
    "    res.save_to_json(\"../data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ed15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def plot_paddleocr_bounding_boxes(image_path, ocr_results):\n",
    "    # Load the original image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for res in ocr_results:\n",
    "        # Get detection polygons and recognition results\n",
    "        dt_polys = res['dt_polys']  # Detection polygons (bounding boxes)\n",
    "        \n",
    "        # Plot each detection\n",
    "        for i, poly in enumerate(dt_polys):\n",
    "            # Convert polygon to numpy array\n",
    "            poly = np.array(poly)\n",
    "            \n",
    "            # Create polygon patch for bounding box\n",
    "            polygon = patches.Polygon(poly, linewidth=2, edgecolor='red', facecolor='none', alpha=0.8)\n",
    "            ax.add_patch(polygon)\n",
    "            \n",
    "    \n",
    "    ax.set_title(\"PaddleOCR Detection and Recognition Results\", fontsize=16)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot your results\n",
    "plot_paddleocr_bounding_boxes(\"../data/input/sd19.jpg\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_polygon_sections(image_path, ocr_results):\n",
    "    \n",
    "    # Load the original image\n",
    "    image = Image.open(image_path)\n",
    "    image_cv = cv2.imread(image_path)  # For OpenCV operations\n",
    "    \n",
    "    extracted_images = []\n",
    "    \n",
    "    for res_idx, res in enumerate(ocr_results):\n",
    "        dt_polys = res['dt_polys']\n",
    "        rec_texts = res.get('rec_texts', [])\n",
    "        \n",
    "        for i, poly in enumerate(dt_polys):\n",
    "            # Convert polygon to numpy array\n",
    "            poly = np.array(poly, dtype=np.int32)\n",
    "            \n",
    "            # Get bounding rectangle\n",
    "            x, y, w, h = cv2.boundingRect(poly)\n",
    "            \n",
    "            # Extract the rectangular region\n",
    "            rect_crop = image_cv[y:y+h, x:x+w]\n",
    "            \n",
    "            # Create a mask for the polygon within the bounding rectangle\n",
    "            mask = np.zeros((h, w), dtype=np.uint8)\n",
    "            poly_shifted = poly - [x, y]  # Shift polygon to new coordinate system\n",
    "            cv2.fillPoly(mask, [poly_shifted], 255)\n",
    "            \n",
    "            # Apply mask to create transparent background\n",
    "            rect_crop_rgba = cv2.cvtColor(rect_crop, cv2.COLOR_BGR2RGBA)\n",
    "            rect_crop_rgba[:, :, 3] = mask  # Set alpha channel\n",
    "            \n",
    "            # Convert back to PIL Image\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(rect_crop_rgba, cv2.COLOR_BGRA2RGBA))\n",
    "            \n",
    "            # Generate filename\n",
    "            text_part = \"\"\n",
    "            if rec_texts and i < len(rec_texts):\n",
    "                # Clean text for filename\n",
    "                text_part = \"\".join(c for c in rec_texts[i] if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "                text_part = text_part.replace(' ', '_')[:20]  # Limit length\n",
    "            \n",
    "            section = f\"section_{res_idx}_{i:03d}_{text_part}.png\"\n",
    "        \n",
    "  \n",
    "            extracted_images.append({\n",
    "                'image': pil_image,\n",
    "                'polygon': poly,\n",
    "                'text': rec_texts[i] if rec_texts and i < len(rec_texts) else \"\",\n",
    "                'section': section\n",
    "            })\n",
    "            \n",
    "    \n",
    "    return extracted_images\n",
    "\n",
    "extracted_sections = extract_polygon_sections(\"../data/input/sd19.jpg\", result)\n",
    "\n",
    "# print name and extracted section\n",
    "for item in extracted_sections:\n",
    "    print(f\"Section: {item['section']}, Text: {item['text']}\")\n",
    "    # plot image\n",
    "    plt.imshow(item['image'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ad508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets compare using a different model the test extraction\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "device_map = \"cpu\"\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", dtype=\"auto\", device_map=device_map, cache_dir=\"../models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de225013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_extraction(img,processor):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": img,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Extract the text from the image.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device_map)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return output_text[0]\n",
    "\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "for item in extracted_sections:\n",
    "    text = qwen_extraction(item['image'],processor)\n",
    "    print(f\"Text: {item['text']}\")\n",
    "    print(f\"Qwen Extracted Text: {text}\")\n",
    "    # plot image\n",
    "    plt.imshow(item['image'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
